{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee4e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a machine learning algorithm that can automate the process (Supervised Learning)\n",
    "#Use correctly labelled data to build an algorithm that can suggest labels for unlabelled lines\n",
    "#Classification problem\n",
    "#Predictions will be probabilites for each label (between 0 and 1)\n",
    "\n",
    "#Budget data: line-item: 'Algebra books for 8th grade student'; labels: 'textbooks', 'math', 'middle school'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the data\n",
    "import pandas as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('TrainingData.csv')\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df['FTE'].dropna())\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n",
    "\n",
    "#Datatypes \n",
    "df.dtypes()\n",
    "\n",
    "#Encode strings as categories\n",
    "df['column'] = df.column.astype('category')\n",
    "\n",
    "#Dummy variable encoding\n",
    "dummies = pd.get_dummies(df[['column']], prefix_sep = '_')\n",
    "\n",
    "# Using lambda function\n",
    "LABELS = ['Function',\n",
    " 'Use',\n",
    " 'Sharing',\n",
    " 'Reporting',\n",
    " 'Student_Type',\n",
    " 'Position_Type',\n",
    " 'Object_Type',\n",
    " 'Pre_K',\n",
    " 'Operating_Status']\n",
    "categorise_labels = lambda x: x.astype('category')\n",
    "df[LABELS] = df[LABELS].apply(categorise_labels, axis = 0)\n",
    "\n",
    "#Counting unique labels\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind = 'bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65515412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do we measure if the algorithm works? (Meauring success)\n",
    "\n",
    "# Computing log loss\n",
    "def compute_log_loss(predicted, actual, eps = 1e-14):\n",
    "    \"\"\" Computes the logarithmic loss between predicted and \n",
    "        actual when these are 1D arrays.\n",
    "    \n",
    "        :param predicted: The predicted probabilites a floats between 0-1\n",
    "        :param actual: The actual binary labels. Either 0 or 1\n",
    "        :param eps (optional): log(0) is inf, so we need to offset our predicted values slightly by eps from 0 to 1.\n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted))\n",
    "              + (1 - actual)\n",
    "              * np.log(1 - predicted)\n",
    "            \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Compute and print log loss for 1st case\n",
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a model\n",
    "\n",
    "##First multi-classs logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier #treat each column of y independently, fitting separate classifier for each of the columns\n",
    "\n",
    "# Split data to just contain numeric columns\n",
    "data_to_train = df[NUMERIC COLUMNS].fillna(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size = 0.2, seed = 123)\n",
    "\n",
    "# Training the model\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "##Predicting on the holdout data\n",
    "holdout = pd.read_csv('HoldoutData.csv', index_col = 0)\n",
    "holdout = holdout[NUMERIC COLUMNS].fillna(-1000)\n",
    "predictions = clf.predict_proba(holdout)\n",
    "\n",
    "##Submitting predictions to CSV\n",
    "prediction_df = pd.DataFrame(columns = pd.get_dummies(df[LABELS], prefix_sep = '__').columns,\n",
    "                            index = holdout.index, data = predictions)\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "score = score_submission(pred_path = 'predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f578f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Processing (NLP)\n",
    "#Tokenisation: splitting strings into segements, store segments as lists\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TOKENS_BASIC = '\\\\\\\\S+(?=\\\\\\\\s+)'\n",
    "df.Program_Description.fillna('', inplace = True)\n",
    "vec_basic = CountVectorizer(token_pattern = TOKENS_BASC)\n",
    "\n",
    "vec_basic.fit(df.Program_Description)\n",
    "msg = 'There are {} tokens in Program Description if tokens are any non-whitesspace'\n",
    "print(msg.format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "\n",
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis = 1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace = True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern = TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines, feature & text preprocessing\n",
    "#Pipeline is a repeatable way to go from raw data to trained model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pl = Pipeline([('imp', Imputer()), \n",
    "               ('clf', OneVsRestClassifier(Logistic Regression()))\n",
    "              ])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['numeric'], \n",
    "                                                    pd.get_dummies(df['label']),\n",
    "                                                    random_state = 2)\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"Accuracy on sample data: \", accuracy)\n",
    "\n",
    "##Preprocessing text features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                    pd.get_dummies(df['label']),\n",
    "                                                    random_state = 2)\n",
    "\n",
    "pl = Pipeline([('vec', CountVectorizer()), \n",
    "               ('clf', OneVsRestClassifier(Logistic Regression()))\n",
    "              ])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"Accuracy on sample data: \", accuracy)\n",
    "\n",
    "##Using both numeric and text data in single pipeline\n",
    "#Write two functions for pipeline preprocessing: 1. Take entire DataFrame, return numeric columns\n",
    "#2. Take entire DataFrame, return text columns\n",
    "#Then, can preprocess numeric and text data in separate pipelines\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['numeric', 'text']], \n",
    "                                                    pd.get_dummies(df['label']),\n",
    "                                                    random_state = 2)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(lambda x: x['numeric'], validate = False)\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate = False)\n",
    "\n",
    "union = FeatureUnion([('numeric', numeric_pipeline),\n",
    "                     ('text', text_pipeline)])\n",
    "\n",
    "numeric_pipeline = Pipeline([('selector', get_numeric_data),\n",
    "                            ('imputer', Imputer())])\n",
    "text_pipeline = Pipeline([('selector', get_text_data),\n",
    "                            ('vectoriser', CountVectorizer())])\n",
    "\n",
    "pl = Pipeline([('union', FeatureUnion([('numeric', numeric_pipeline), ('text', text_pipeline)])),\n",
    "              ('clf', OneVsRestClassifier(LogisticRegression())])\n",
    "\n",
    "\n",
    "\n",
    "# Example Practice\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae11e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pipeline on School Budget dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    df[NON_LABELS], dummy_labels, 0.2)\n",
    "\n",
    "\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate = False)\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate = False)\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "            ('selector', get_numeric_data), \n",
    "            ('imputer', Imputer())])),\n",
    "        ('text_features', Pipeline([\n",
    "            ('selector', get_text_data),\n",
    "            ('vectoriser', CountVectorizer())]))\n",
    "    ])\n",
    "    ),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "# Pipeline code from example\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)\n",
    "\n",
    "#Edit model step in pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "            ('selector', get_numeric_data), \n",
    "            ('imputer', Imputer())])),\n",
    "        ('text_features', Pipeline([\n",
    "            ('selector', get_text_data),\n",
    "            ('vectoriser', CountVectorizer())]))\n",
    "    ])\n",
    "    ),\n",
    "    ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Full Process\n",
    "\n",
    "##Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_vector = combine_text_columns(X_train) #create the text vector\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' #create token pattern\n",
    "vec = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC, ngram_range = (1, 2))\n",
    "vec.fit(text_vector)\n",
    "\n",
    "##Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "##Interaction terms (statistical tools), interaction terms mathematiically describe when tokens appear together\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree = 2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "##Hashing\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ce87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Winning Pipeline\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     non_negative=True, norm=None, binary=False,\n",
    "                                                     ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
